{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab 3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1h35aMtoTI7b",
        "outputId": "4452bc53-7664-4427-a8e3-798736d7ecd6"
      },
      "source": [
        "!git clone https://github.com/chrishendra93/MI_Workshop\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'MI_Workshop'...\n",
            "remote: Enumerating objects: 86, done.\u001b[K\n",
            "remote: Counting objects: 100% (86/86), done.\u001b[K\n",
            "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
            "remote: Total 86 (delta 38), reused 60 (delta 22), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (86/86), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sd-0YK5MzfcZ"
      },
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68iJXH7HEdFd"
      },
      "source": [
        "# Setting random seed so that everything is replicable\n",
        "\n",
        "np.random.seed(0)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0wmIe0PEfVs",
        "outputId": "453cf4f8-9c34-4192-a911-cd63ac71a474"
      },
      "source": [
        "root_dir = \"/content/MI_Workshop/mnist_clr\"\n",
        "data_dir = \"/content/MI_Workshop/lab_3\"\n",
        "\n",
        "mnist_columns = [\"label\"] + [\"features_{}\".format(i) for i in range(28 ** 2)]\n",
        "mnist_train = pd.read_csv(\"./sample_data/mnist_train_small.csv\", names=mnist_columns)\n",
        "mnist_test = pd.read_csv(\"./sample_data/mnist_test.csv\", names=mnist_columns)\n",
        "mnist_arr_train = mnist_train[[\"features_{}\".format(i) for i in range(28 ** 2)]].values\n",
        "mnist_arr_test = mnist_test[[\"features_{}\".format(i) for i in range(28 ** 2)]].values\n",
        "\n",
        "X_train = np.load(os.path.join(root_dir, \"train_features.npy\"))\n",
        "X_test = np.load(os.path.join(root_dir, \"test_features.npy\"))\n",
        "y_train = np.load(os.path.join(root_dir, \"train_labels.npy\"))\n",
        "y_test = np.load(os.path.join(root_dir, \"test_labels.npy\"))\n",
        "\n",
        "print(np.all(y_train == mnist_train[\"label\"].values))\n",
        "print(np.all(y_test == mnist_test[\"label\"].values))\n",
        "\n",
        "train_bags, train_labels = np.load(os.path.join(data_dir, \"train_bags.npy\"), allow_pickle=True), np.load(os.path.join(data_dir, \"train_labels.npy\"),allow_pickle=True)\n",
        "test_bags, test_labels = np.load(os.path.join(data_dir, \"test_bags.npy\"), allow_pickle=True), np.load(os.path.join(data_dir, \"test_labels.npy\"), allow_pickle=True)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hs3FBTyQE8Pp"
      },
      "source": [
        "# Visualize distribution of labels in train and test sets\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Azt-0CNME9d0"
      },
      "source": [
        "# Visualize distribution of instances in train and test sets\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfCMIJMc0Wgg"
      },
      "source": [
        "def visualize_bags(mnist_arr, bags, bag_labels, n_bags):\n",
        "  pos_bags = np.argwhere(bag_labels == 1).flatten()\n",
        "  neg_bags = np.argwhere(bag_labels == 0).flatten()\n",
        "  n_pos = (n_bags // 2 )\n",
        "  n_neg = n_bags - n_pos\n",
        "  sampled_indices = np.concatenate([np.random.choice(pos_bags, n_pos, replace=False), np.random.choice(neg_bags, n_neg, replace=False)])\n",
        "  sampled_bags = bags[sampled_indices]\n",
        "  sampled_bag_labels = bag_labels[sampled_indices] \n",
        "  max_instances_num = np.max([len(bag) for bag in sampled_bags])\n",
        "  _, axes = plt.subplots(len(sampled_bags), max_instances_num,\n",
        "                         figsize=(10 * len(sampled_bags), \n",
        "                                  10 * max_instances_num))\n",
        "  for idx, bag, bag_label in zip(np.arange(n_bags), sampled_bags, sampled_bag_labels):\n",
        "    ax = axes[idx, :]\n",
        "    instance_size = None\n",
        "    for i in range(max_instances_num):\n",
        "      if i >= len(bag):\n",
        "        ax[i].imshow(np.zeros(instance_size)) # Pad with empty images if bag has fewer instances than max\n",
        "      else:\n",
        "        img = mnist_arr[bag[i]]\n",
        "        if instance_size is None:\n",
        "          w = int(np.sqrt(len(img)))\n",
        "          instance_size = (w, w)\n",
        "        ax[i].imshow(img.reshape(instance_size))\n",
        "      if i == max_instances_num // 2:\n",
        "        ax[i].set_title('Bag Label: {}'.format(bag_label), fontsize=50)\n",
        "  plt.subplots_adjust(bottom=0.1, top=0.3, hspace=0.2)\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vADXbhoy0LM6"
      },
      "source": [
        "# Visualize instances in positive bags\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXT6FaKYFos0"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import BCELoss\n",
        "from torch.optim import LBFGS, Adam\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFYoZ0_cGP7Y"
      },
      "source": [
        "class LogisticRegressionMI(nn.Module):\n",
        "\n",
        "  def __init__(self, n_dim, mode='max'):\n",
        "    super(LogisticRegressionMI, self).__init__()\n",
        "    if mode not in ('max', 'mean'):\n",
        "      raise ValueError(\"Invalid mode {}, must be one of max or mean\".format(mode))\n",
        "    self.mode = mode\n",
        "    self.encoder = nn.Linear(n_dim, 1)\n",
        "    \n",
        "  def forward(self, x, indices):\n",
        "    x = self.encoder(x)\n",
        "    x = torch.sigmoid(x)\n",
        "    if self.mode == 'max':\n",
        "      x = torch.stack([torch.max(x[idx]) for idx in indices])\n",
        "    else:\n",
        "      x = torch.stack([torch.mean(x[idx]) for idx in indices])\n",
        "    return x\n",
        "  \n",
        "  def get_max_indices(self, x, indices):\n",
        "    x = self.encoder(x)\n",
        "    x = torch.sigmoid(x)\n",
        "    pred, max_indices = [], []\n",
        "    for idx in indices:\n",
        "      max_idx = torch.argmax(x[idx])\n",
        "      max_indices.append(idx[max_idx])\n",
        "      pred.append(x[idx][max_idx])\n",
        "    return torch.cat(pred), np.array(max_indices)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69IpL8GUIYx9"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, roc_curve, precision_recall_curve, auc\n",
        "from scipy.stats import mode\n",
        "\n",
        "def get_roc_auc(y_true, y_pred):\n",
        "    fpr, tpr, _  = roc_curve(y_true, y_pred)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    return roc_auc\n",
        "\n",
        "\n",
        "def get_pr_auc(y_true, y_pred):\n",
        "    precision, recall, _ = precision_recall_curve(y_true, y_pred, pos_label=1)\n",
        "    pr_auc = auc(recall, precision)\n",
        "    return pr_auc\n",
        "\n",
        "\n",
        "def get_accuracy(y_true, y_pred):\n",
        "    return balanced_accuracy_score(y_true, y_pred)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFPi7CVBHDA7",
        "outputId": "a994a71b-3ca1-46b5-ffd9-3eb80449a25e"
      },
      "source": [
        "# logit_mi = LogisticRegressionMI(64, mode='mean')\n",
        "# criterion = BCELoss()\n",
        "# optimizer = LBFGS(logit_mi.parameters(), lr=0.001, max_iter=1000)\n",
        "# logit_mi.train()\n",
        "\n",
        "\n",
        "# def closure():\n",
        "#     optimizer.zero_grad()\n",
        "#     output = logit_mi(torch.Tensor(X_train), train_bags)\n",
        "#     loss = criterion(output, torch.Tensor(train_labels))\n",
        "#     loss.backward()\n",
        "#     return loss\n",
        "\n",
        "# optimizer.step(closure) \n",
        "\n",
        "# logit_mi.eval()\n",
        "# with torch.no_grad():\n",
        "#   y_pred_logit_mi_test = logit_mi(torch.Tensor(X_test), test_bags).detach().numpy()\n",
        "#   print(\"ROC AUC: {}\".format(get_roc_auc(test_labels, y_pred_logit_mi_test)))\n",
        "#   print(\"PR AUC: {}\".format(get_pr_auc(test_labels, y_pred_logit_mi_test)))\n",
        "#   print(\"Accuracy Score: {}\".format(accuracy_score(test_labels, y_pred_logit_mi_test >= 0.5)))\n",
        "#   print(\"Balanced Accuracy Score: {}\".format(balanced_accuracy_score(test_labels, y_pred_logit_mi_test >= 0.5)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROC AUC: 0.8444277901716553\n",
            "PR AUC: 0.3773344528695057\n",
            "Accuracy Score: 0.8806306306306306\n",
            "Balanced Accuracy Score: 0.49974437627811863\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DfhEnGdhPob"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "\n",
        "torch.random.manual_seed(0)\n",
        "\n",
        "class nnMI(nn.Module):\n",
        "\n",
        "  def __init__(self, encoder, decoder, mode='max'):\n",
        "    super(nnMI, self).__init__()\n",
        "    if mode not in ('max', 'mean'):\n",
        "      raise ValueError(\"Invalid mode {}, must be one of max or mean\".format(mode))\n",
        "    self.mode = mode\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    \n",
        "  def forward(self, x, indices):\n",
        "    x = self.encoder(x)\n",
        "    if self.mode == 'max':\n",
        "      x = torch.stack([torch.max(x[idx], axis=0).values for idx in indices])\n",
        "    else:\n",
        "      x = torch.stack([torch.mean(x[idx], axis=0).values for idx in indices])\n",
        "    x = self.decoder(x)\n",
        "    return x"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXDArRCeKLPp",
        "outputId": "9e5575a7-b2c0-49c8-dfee-2e34f6590100"
      },
      "source": [
        "device = 'cpu'\n",
        "\n",
        "encoder = nn.Sequential(*[nn.Linear(64, 128), nn.ReLU(),\n",
        "                          nn.Linear(128, 64), nn.ReLU()])\n",
        "\n",
        "decoder = nn.Sequential(*[nn.Linear(64, 128), nn.ReLU(),\n",
        "                          nn.Linear(128, 64), nn.ReLU(),\n",
        "                          nn.Linear(64, 1)])\n",
        "\n",
        "nn_mi = nnMI(encoder, decoder, mode='max').to(device)\n",
        "\n",
        "\n",
        "# Computing class weight for BCEWithLogitsLoss\n",
        "# The loss requires weight for each individual data point so we have to create a tensor of size (N, )\n",
        "# Here N is the number of bags in the training set and each entry corresponds to the weight that we want to assign to that particular entr\n",
        "\n",
        "classes, counts = np.unique(train_labels, return_counts=True)\n",
        "class_weights = 1./torch.tensor(counts, dtype=torch.float)\n",
        "class_weights = class_weights / class_weights.sum()\n",
        "\n",
        "weights = torch.zeros(len(train_labels))\n",
        "weights[np.argwhere(train_labels == 0)] = class_weights[0]\n",
        "weights[np.argwhere(train_labels == 1)] = class_weights[1]\n",
        "\n",
        "# Initialize loss function, optimizer and number of epochs\n",
        "\n",
        "\n",
        "# Implement the torch training loop in here\n",
        "\n",
        "nn_mi.eval()\n",
        "with torch.no_grad():\n",
        "  y_pred_nn_test = torch.sigmoid(nn_mi(torch.Tensor(X_test).to(device), test_bags)).detach().cpu().numpy().flatten()\n",
        "  print(\"ROC AUC: {}\".format(get_roc_auc(test_labels, y_pred_nn_test)))\n",
        "  print(\"PR AUC: {}\".format(get_pr_auc(test_labels, y_pred_nn_test)))\n",
        "  print(\"Accuracy Score: {}\".format(accuracy_score(test_labels, y_pred_nn_test >= 0.5)))\n",
        "  print(\"Balanced Accuracy Score: {}\".format(balanced_accuracy_score(test_labels, y_pred_nn_test >= 0.5)))\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROC AUC: 0.9306891770465391\n",
            "PR AUC: 0.6986790905666337\n",
            "Accuracy Score: 0.918018018018018\n",
            "Balanced Accuracy Score: 0.8420710169176426\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMlPgzITwCke"
      },
      "source": [
        "## Implement a dataset class here\n",
        "\n",
        "# class MIDataSet(Dataset):\n",
        "\n",
        "#   def __init__(self, X, bags, labels):\n",
        "\n",
        "\n",
        "#   def __getitem__(self, idx):\n",
        "\n",
        "  \n",
        "#   def __len__(self):\n",
        "\n",
        "\n",
        "# def collate_fn(batch):\n",
        "#   X = torch.cat([sample[0] for sample in batch])\n",
        "#   y = torch.cat([sample[1] for sample in batch])\n",
        "#   indices = []\n",
        "#   i = 0\n",
        "#   for sample in batch:\n",
        "#     n_instances = sample[2]\n",
        "#     indices.append(np.arange(i, i + n_instances))\n",
        "#     i += n_instances\n",
        "#   return X, y, indices\n",
        "\n",
        "# train_ds = MIDataSet(X_train, train_bags, train_labels)\n",
        "# train_dl = DataLoader(train_ds, num_workers=10, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# test_ds = MIDataSet(X_test, test_bags, test_labels)\n",
        "# test_dl = DataLoader(test_ds, num_workers=10, batch_size=64, shuffle=False, collate_fn=collate_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmHAneFKwRdt"
      },
      "source": [
        "# Implement a training loop just as before but with torch "
      ],
      "execution_count": 22,
      "outputs": []
    }
  ]
}